{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0757c90a-c834-4324-868f-95937ffa7591",
   "metadata": {},
   "source": [
    "### Step 1: Load the Exported Model\n",
    "- Load the trained model from `friends_classifier.pkl`\n",
    "- Run a quick test prediction to confirm it works\n",
    "- This sets up the model for use in the Gradio app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "234606b3-eea2-4630-82ad-327fef5b38ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Sneha\n",
      "Confidence: 0.9999\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "from fastai.vision.all import *\n",
    "\n",
    "# Load the exported model\n",
    "learn = load_learner('models/friends_classifier.pkl')\n",
    "\n",
    "# Quick test prediction\n",
    "img = PILImage.create(r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample5.jpg\")\n",
    "pred_class, pred_idx, probs = learn.predict(img)\n",
    "\n",
    "print(f\"Prediction: {pred_class}\")\n",
    "print(f\"Confidence: {probs[pred_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f8ba3-7823-405b-9a29-3f43959f91b9",
   "metadata": {},
   "source": [
    "### Step 2: Define the Prediction Function\n",
    "- Create a function that takes a PIL image as input\n",
    "- Resize it to match training size (224Ã—224)\n",
    "- Use `learn.predict()` to get the predicted class and probabilities\n",
    "- Return a dictionary of class â†’ probability for Gradio display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce6ab52a-6f65-4961-8975-9333a1c2df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "# Define your 5 categories (must match training labels)\n",
    "categories = ('Archit', 'Hardik', 'Kabir', 'Sahib', 'Sneha')\n",
    "\n",
    "# Prediction function for Gradio\n",
    "def classify_image(img):\n",
    "    img = img.resize((224,224))   # Resize to match training size\n",
    "    with learn.no_bar():          # Disable progress bar\n",
    "        pred_class, idx, probs = learn.predict(img)\n",
    "    return dict(zip(categories, map(float, probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca92727-acd0-4ca3-8498-67a3f4d0c4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b202705-3624-4c5d-b4cc-260eb106ac5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Archit': 0.99994957447052,\n",
       " 'Hardik': 3.0989265269454336e-06,\n",
       " 'Kabir': 1.5812629499123432e-05,\n",
       " 'Sahib': 3.107286516979002e-08,\n",
       " 'Sneha': 3.144101719954051e-05}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_image(Image.open(r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample1.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f44254ae-a89a-493e-add2-585708a3833e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Archit': 5.327066219251719e-07,\n",
       " 'Hardik': 1.1288627320027445e-05,\n",
       " 'Kabir': 0.9998114705085754,\n",
       " 'Sahib': 0.00016928912373259664,\n",
       " 'Sneha': 7.531840310548432e-06}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_image(Image.open(r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample3.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c56de37-cef5-49bc-8f4c-f91241ab8da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Archit': 2.7004007918662865e-09,\n",
       " 'Hardik': 0.00010650106560206041,\n",
       " 'Kabir': 5.413273242993455e-07,\n",
       " 'Sahib': 1.0687258509278763e-05,\n",
       " 'Sneha': 0.9998822212219238}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify_image(Image.open(r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample5.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e13ad-176c-497a-bac2-0d910da4792e",
   "metadata": {},
   "source": [
    "### Step 3: Build the Gradio Interface\n",
    "- Use `gr.Interface()` to connect your prediction function to a UI\n",
    "- Input: image (PIL)\n",
    "- Output: label with top 5 class probabilities\n",
    "- Add 5 example images from your `Examples` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ff5d53-c69e-406e-9797-d87ce6d2acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 403, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\uvicorn\\middleware\\proxy_headers.py\", line 60, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\fastapi\\applications.py\", line 1134, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 186, in __call__\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 164, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\gradio\\brotli_middleware.py\", line 74, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\gradio\\route_utils.py\", line 882, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\middleware\\exceptions.py\", line 63, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\fastapi\\middleware\\asyncexitstack.py\", line 18, in __call__\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\routing.py\", line 716, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\routing.py\", line 736, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\routing.py\", line 290, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\fastapi\\routing.py\", line 124, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\_exception_handler.py\", line 53, in wrapped_app\n",
      "    raise exc\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\_exception_handler.py\", line 42, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\fastapi\\routing.py\", line 111, in app\n",
      "    await response(scope, receive, send)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\responses.py\", line 369, in __call__\n",
      "    await self._handle_simple(send, send_header_only, send_pathsend)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\responses.py\", line 400, in _handle_simple\n",
      "    await send({\"type\": \"http.response.body\", \"body\": chunk, \"more_body\": more_body})\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\_exception_handler.py\", line 39, in sender\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\starlette\\middleware\\errors.py\", line 161, in _send\n",
      "    await send(message)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\uvicorn\\protocols\\http\\h11_impl.py\", line 507, in send\n",
      "    output = self.conn.send(event=h11.EndOfMessage())\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\h11\\_connection.py\", line 538, in send\n",
      "    data_list = self.send_with_data_passthrough(event)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\h11\\_connection.py\", line 571, in send_with_data_passthrough\n",
      "    writer(event, data_list.append)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\h11\\_writers.py\", line 67, in __call__\n",
      "    self.send_eom(event.headers, write)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\h11\\_writers.py\", line 96, in send_eom\n",
      "    raise LocalProtocolError(\"Too little data for declared Content-Length\")\n",
      "h11._util.LocalProtocolError: Too little data for declared Content-Length\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Archit\\anaconda3\\envs\\fc\\lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"C:\\Users\\Archit\\AppData\\Local\\Temp\\ipykernel_34544\\3200682577.py\", line 7, in classify_image\n",
      "    img = img.resize((224,224))   # Resize to match training size\n",
      "AttributeError: 'NoneType' object has no attribute 'resize'\n"
     ]
    }
   ],
   "source": [
    "#|export\n",
    "import gradio as gr\n",
    "\n",
    "# Define input and output components\n",
    "image = gr.Image(type='pil')\n",
    "label = gr.Label(num_top_classes=5)\n",
    "\n",
    "# Add example images from your local folder\n",
    "examples = [\n",
    "    r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample1.jpg\",\n",
    "    r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample2.jpg\",\n",
    "    r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample3.jpg\",\n",
    "    r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample4.jpg\",\n",
    "    r\"C:\\Users\\Archit\\Desktop\\Frendclassifier\\Images\\Examples\\sample5.jpg\"\n",
    "]\n",
    "\n",
    "# Build the Gradio interface\n",
    "intf = gr.Interface(\n",
    "    fn=classify_image,\n",
    "    inputs=image,\n",
    "    outputs=label,\n",
    "    examples=examples,\n",
    "    title=\"Friends Classifier\",\n",
    "    description=\"Upload a face image of Archit, Hardik, Kabir, Sahib, or Sneha to see the prediction.\"\n",
    ")\n",
    "\n",
    "# Launch the app\n",
    "intf.launch(inline=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679c9a0-50da-4a5f-ad88-b283d3a8af3f",
   "metadata": {},
   "source": [
    "##  Wrap-Up\n",
    "\n",
    "- We successfully deployed our trained `friends_classifier.pkl` model as a Gradio app.  \n",
    "- The app allows uploading a single face image and returns probabilities for Archit, Hardik, Kabir, Sahib, and Sneha.  \n",
    "- Accuracy on validation was ~92â€“94%, and the app predictions align well with test images.  \n",
    "\n",
    "### Limitations\n",
    "- The app currently supports **single-face images only**.  \n",
    "- Group photos are misclassified because the model was not trained for multiple faces.  \n",
    "\n",
    "### Next Steps\n",
    "- Build a new notebook for **multi-face detection + classification** using a face detection library.  \n",
    "- Enhance the UI with probability bar charts and deploy to Hugging Face Spaces for sharing.  \n",
    "\n",
    "This completes Phaseâ€¯3 (single-face deployment). ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757d623-46b1-47d3-b0d7-77019d665d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
